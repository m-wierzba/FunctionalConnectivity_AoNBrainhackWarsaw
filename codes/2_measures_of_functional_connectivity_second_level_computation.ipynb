{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http://pandas.pydata.org/pandas-docs/version/0.15.2/10min.html\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.covariance import GraphLassoCV\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from scipy import stats\n",
    "import matplotlib.patches as patches\n",
    "import scipy.io\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels_Glasser = pd.read_csv(\"labels_Glasser.csv\",header=None)[0].tolist()\n",
    "Nvars = len(labels_Glasser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give a path to the folder with the resting state datasets, day 1:\n",
    "condition = 'restingstate_day2'  # restingstate_day1 / restingstate_day2 / WM / MOTOR\n",
    "if condition == 'restingstate_day1':\n",
    "    T = 2389\n",
    "if condition == 'restingstate_day2':\n",
    "    T = 2389\n",
    "if condition == 'WM':    \n",
    "    T = 799\n",
    "if condition == 'MOTOR':\n",
    "    T = 557\n",
    "\n",
    "path = '/Users/nataliabielczyk/Desktop/Brainhack/PIPELINE/datasets/HCP_parcellated_smoothing5_100subjects_Glasser_' + condition + '/'\n",
    "list = os.listdir(path) # dir is your directory path\n",
    "number_files = len(list)\n",
    "\n",
    "Nsubjects = 0\n",
    "indexes_subjects = []\n",
    "for ind in range(number_files):\n",
    "    filename = path + list[ind]\n",
    "    if filename[-3:] == 'csv':\n",
    "        Nsubjects = Nsubjects + 1\n",
    "        indexes_subjects.append(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n"
     ]
    }
   ],
   "source": [
    "# [01] Pearson's r:\n",
    "# compute Pearson's r for all the subjects:\n",
    "pearsonr_all = np.zeros((Nsubjects,Nvars,Nvars))\n",
    "k = 0\n",
    "for ind in range(number_files):\n",
    "    filename = path + list[ind]\n",
    "    if filename[-3:] == 'csv':\n",
    "        \n",
    "        df1    = pd.read_csv(filename)\n",
    "        data1  = df1.values\n",
    "        Nvars  = data1.shape[1] \n",
    "        \n",
    "        ## shorten the datasets to 2 X 400, or 2 x 279 samples, in order to make them comparable with the visual stimulation task:\n",
    "        #Nshort = 400\n",
    "        #list1 = np.ndarray.tolist(np.arange(Nshort))\n",
    "        #list2 = np.ndarray.tolist(1200+np.arange(Nshort))\n",
    "        #indexes = list1 + list2       \n",
    "        #data1 = data1[indexes,:]\n",
    "        \n",
    "        pearsonr = np.zeros((Nvars,Nvars))\n",
    "        pearsonr_p = np.zeros((Nvars,Nvars))\n",
    "        for ind1 in range(Nvars):\n",
    "            for ind2 in range(Nvars):\n",
    "                (pearsonr[ind1,ind2], pearsonr_p[ind1,ind2]) = scipy.stats.pearsonr(data1[:,ind1], data1[:,ind2])\n",
    "        \n",
    "        pearsonr_all[k,:,:] = pearsonr\n",
    "        k = k + 1   \n",
    "        print k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "foldername = 'output_second_level/'\n",
    "if not os.path.exists(foldername):\n",
    "    os.makedirs(foldername)\n",
    "    \n",
    "filename = foldername + 'pearsonr_all_' + condition + '.mat'\n",
    "scipy.io.savemat(filename, {'pearsonr_all': pearsonr_all})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n"
     ]
    }
   ],
   "source": [
    "# surrogate version of Pearson's r:\n",
    "pearsonr_all_surrogate = np.zeros((Nsubjects,Nvars,Nvars))\n",
    "\n",
    "for ind in range(Nsubjects):\n",
    "    print ind\n",
    "    # combine a surrogate subject from multiple labels:\n",
    "    surrogate_subject = np.zeros((T,Nvars))\n",
    "    # choose time series for every ROI from random subjects:\n",
    "    for ind1 in range(Nvars):\n",
    "        ind2                      = random.sample(range(1, Nsubjects), 1)[0] # random subject number\n",
    "        filename                  = path + list[indexes_subjects[ind2]]\n",
    "        df1                       = pd.read_csv(filename)\n",
    "        data1                     = df1.values \n",
    "        surrogate_subject[:,ind1] = data1[:,ind1]\n",
    "        \n",
    "    # shorten the datasets to 2 X 400, or 2 x 279 samples, in order to make them comparable with the visual stimulation task:\n",
    "    #Nshort = 400\n",
    "    #list1 = np.ndarray.tolist(np.arange(Nshort))\n",
    "    #list2 = np.ndarray.tolist(1200+np.arange(Nshort))\n",
    "    #indexes = list1 + list2       \n",
    "    #data1 = data1[indexes,:]\n",
    "        \n",
    "    pearsonr = np.zeros((Nvars,Nvars))\n",
    "    pearsonr_p = np.zeros((Nvars,Nvars))\n",
    "    for ind1 in range(Nvars):\n",
    "        for ind2 in range(Nvars):\n",
    "            (pearsonr[ind1,ind2], pearsonr_p[ind1,ind2]) = scipy.stats.pearsonr(data1[:,ind1], data1[:,ind2])\n",
    "      \n",
    "    pearsonr_all_surrogate[ind,:,:] = pearsonr  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "foldername = 'output_second_level/'\n",
    "if not os.path.exists(foldername):\n",
    "    os.makedirs(foldername)\n",
    "    \n",
    "filename = foldername + 'pearsonr_all_surrogate_' + condition + '.mat'\n",
    "scipy.io.savemat(filename, {'pearsonr_all_surrogate': pearsonr_all_surrogate})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-139-7bc8fb791200>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'csv'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mdf1\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mdata1\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mNvars\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mdata1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nataliabielczyk/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nataliabielczyk/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nataliabielczyk/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    980\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'skipfooter not supported for iteration'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 982\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'as_recarray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nataliabielczyk/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1717\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1718\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1719\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1720\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1721\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read (pandas/_libs/parsers.c:10862)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory (pandas/_libs/parsers.c:11138)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows (pandas/_libs/parsers.c:12175)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data (pandas/_libs/parsers.c:14136)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens (pandas/_libs/parsers.c:14858)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype (pandas/_libs/parsers.c:15629)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/Users/nataliabielczyk/anaconda/lib/python2.7/site-packages/pandas/core/dtypes/common.pyc\u001b[0m in \u001b[0;36mis_integer_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mis_integer_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    738\u001b[0m     \"\"\"\n\u001b[1;32m    739\u001b[0m     \u001b[0mCheck\u001b[0m \u001b[0mwhether\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mprovided\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mof\u001b[0m \u001b[0man\u001b[0m \u001b[0minteger\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# [02] partial correlation:\n",
    "# compute partial correlation for all the subjects:\n",
    "partial_correlation_all = np.zeros((Nsubjects,Nvars,Nvars))\n",
    "k = 0\n",
    "for ind in range(number_files):\n",
    "    filename = path + list[ind]\n",
    "    if filename[-3:] == 'csv':\n",
    "        \n",
    "        df1    = pd.read_csv(filename)\n",
    "        data1  = df1.values\n",
    "        Nvars  = data1.shape[1]  \n",
    "        \n",
    "        # shorten the datasets to 2 X 400, or 2 x 279 samples, in order to make them comparable with the visual stimulation task:\n",
    "        Nshort = 400\n",
    "        list1 = np.ndarray.tolist(np.arange(Nshort))\n",
    "        list2 = np.ndarray.tolist(1200+np.arange(Nshort))\n",
    "        indexes = list1 + list2       \n",
    "        data1 = data1[indexes,:]\n",
    "        \n",
    "        covariance_matrix = np.cov(np.transpose(data1))\n",
    "        inv_cov = np.linalg.inv(covariance_matrix)\n",
    "        partial_correlation = np.zeros((Nvars,Nvars))\n",
    "        for ind1 in range(Nvars):\n",
    "            for ind2 in range(Nvars):\n",
    "                partial_correlation[ind1,ind2] = - inv_cov[ind1,ind2]/np.sqrt((inv_cov[ind1,ind1])*(inv_cov[ind2,ind2]))\n",
    "        \n",
    "        partial_correlation_all[k,:,:] = partial_correlation\n",
    "        k = k + 1  \n",
    "        print k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "foldername = 'output_second_level/'\n",
    "if not os.path.exists(foldername):\n",
    "    os.makedirs(foldername)\n",
    "    \n",
    "filename = foldername + 'partial_correlation_all_' + condition + '_800samples.mat'\n",
    "scipy.io.savemat(filename, {'partial_correlation_all': partial_correlation_all})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n"
     ]
    }
   ],
   "source": [
    "# surrogate version of partial correlation:\n",
    "partial_correlation_all_surrogate = np.zeros((Nsubjects,Nvars,Nvars))\n",
    "\n",
    "for ind in range(Nsubjects):\n",
    "    print ind\n",
    "    # combine a surrogate subject from multiple labels:\n",
    "    surrogate_subject = np.zeros((T,Nvars))\n",
    "    # choose time series for every ROI from random subjects:\n",
    "    for ind1 in range(Nvars):\n",
    "        ind2                      = random.sample(range(1, Nsubjects), 1)[0] # random subject number\n",
    "        filename                  = path + list[indexes_subjects[ind2]]\n",
    "        df1                       = pd.read_csv(filename)\n",
    "        data1                     = df1.values \n",
    "        surrogate_subject[:,ind1] = data1[:,ind1]\n",
    "        \n",
    "    # shorten the datasets to 2 X 400, or 2 x 279 samples, in order to make them comparable with the visual stimulation task:\n",
    "    Nshort = 279\n",
    "    list1 = np.ndarray.tolist(np.arange(Nshort))\n",
    "    list2 = np.ndarray.tolist(1200+np.arange(Nshort))\n",
    "    indexes = list1 + list2       \n",
    "    data1 = data1[indexes,:]\n",
    "        \n",
    "    covariance_matrix = np.cov(np.transpose(data1))\n",
    "    inv_cov = np.linalg.inv(covariance_matrix)\n",
    "    partial_correlation = np.zeros((Nvars,Nvars))\n",
    "    for ind1 in range(Nvars):\n",
    "        for ind2 in range(Nvars):\n",
    "            partial_correlation[ind1,ind2] = - inv_cov[ind1,ind2]/np.sqrt((inv_cov[ind1,ind1])*(inv_cov[ind2,ind2]))\n",
    "        \n",
    "    partial_correlation_all_surrogate[ind,:,:] = partial_correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "foldername = 'output_second_level/'\n",
    "if not os.path.exists(foldername):\n",
    "    os.makedirs(foldername)\n",
    "    \n",
    "filename = foldername + 'partial_correlation_all_surrogate_' + condition + '_558samples.mat'\n",
    "scipy.io.savemat(filename, {'partial_correlation_all_surrogate': partial_correlation_all_surrogate})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n"
     ]
    }
   ],
   "source": [
    "# [04] mutual information:\n",
    "# compute mutual information for all the subjects:\n",
    "from sklearn.metrics import mutual_info_score\n",
    "def calc_MI(X,Y,bins):\n",
    "   c_XY = np.histogram2d(X,Y,bins)[0]\n",
    "   c_X = np.histogram(X,bins)[0]\n",
    "   c_Y = np.histogram(Y,bins)[0]\n",
    "\n",
    "   H_X = shan_entropy(c_X)\n",
    "   H_Y = shan_entropy(c_Y)\n",
    "   H_XY = shan_entropy(c_XY)\n",
    "\n",
    "   MI = H_X + H_Y - H_XY\n",
    "   return MI\n",
    "def shan_entropy(c):\n",
    "    c_normalized = c / float(np.sum(c))\n",
    "    c_normalized = c_normalized[np.nonzero(c_normalized)]\n",
    "    H = -sum(c_normalized* np.log2(c_normalized))  \n",
    "    return H\n",
    "\n",
    "bins = 10\n",
    "mutual_information_all = np.zeros((Nsubjects,Nvars,Nvars))\n",
    "k = 0\n",
    "for ind in range(number_files):\n",
    "    filename = path + list[ind]\n",
    "    if filename[-3:] =='csv':\n",
    "        \n",
    "        df1    = pd.read_csv(filename)\n",
    "        data1  = df1.values        \n",
    "        Nvars  = data1.shape[1]  \n",
    "        \n",
    "        # shorten the datasets to 2 X 400, or 2 x 279 samples, in order to make them comparable with the visual stimulation task:\n",
    "        Nshort = 400\n",
    "        list1 = np.ndarray.tolist(np.arange(Nshort))\n",
    "        list2 = np.ndarray.tolist(1200+np.arange(Nshort))\n",
    "        indexes = list1 + list2       \n",
    "        data1 = data1[indexes,:]\n",
    "        \n",
    "        matMI = np.zeros((Nvars,Nvars))\n",
    "\n",
    "        for ix in np.arange(Nvars):\n",
    "            for jx in np.arange(ix+1,Nvars):\n",
    "                matMI[ix,jx] = calc_MI(data1[:,ix], data1[:,jx], bins)\n",
    "                matMI[jx,ix] = matMI[ix,jx]\n",
    "        \n",
    "        mutual_information_all[k,:,:] = matMI\n",
    "        k = k + 1  \n",
    "        print k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "foldername = 'output_second_level/'\n",
    "if not os.path.exists(foldername):\n",
    "    os.makedirs(foldername)\n",
    "    \n",
    "filename = foldername + 'mutual_information_all_' + condition + '_800samples.mat'\n",
    "scipy.io.savemat(filename, {'mutual_information_all': mutual_information_all})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n"
     ]
    }
   ],
   "source": [
    "# [04b] permutation testing for mutual information:\n",
    "# create surrogate data:\n",
    "mutual_information_all_surrogate = np.zeros((mutual_information_all.shape))\n",
    "\n",
    "# compute mutual information for all the subjects:\n",
    "from sklearn.metrics import mutual_info_score\n",
    "def calc_MI(X,Y,bins):\n",
    "   c_XY = np.histogram2d(X,Y,bins)[0]\n",
    "   c_X = np.histogram(X,bins)[0]\n",
    "   c_Y = np.histogram(Y,bins)[0]\n",
    "\n",
    "   H_X = shan_entropy(c_X)\n",
    "   H_Y = shan_entropy(c_Y)\n",
    "   H_XY = shan_entropy(c_XY)\n",
    "\n",
    "   MI = H_X + H_Y - H_XY\n",
    "   return MI\n",
    "\n",
    "def shan_entropy(c):\n",
    "    c_normalized = c / float(np.sum(c))\n",
    "    c_normalized = c_normalized[np.nonzero(c_normalized)]\n",
    "    H = -sum(c_normalized* np.log2(c_normalized))  \n",
    "    return H\n",
    "\n",
    "bins = 10\n",
    "for ind in range(Nsubjects):\n",
    "    print ind\n",
    "    # combine a surrogate subject from multiple labels:\n",
    "    surrogate_subject = np.zeros((T,Nvars))\n",
    "    # choose time series for every ROI from random subjects:\n",
    "    for ind1 in range(Nvars):\n",
    "        ind2                      = random.sample(range(1, Nsubjects), 1)[0] # random subject number\n",
    "        filename                  = path + list[indexes_subjects[ind2]]\n",
    "        df1                       = pd.read_csv(filename)\n",
    "        data1                     = df1.values \n",
    "        #print filename\n",
    "        #print data1.shape\n",
    "        #print surrogate_subject.shape\n",
    "        surrogate_subject[:,ind1] = data1[:,ind1]\n",
    "        \n",
    "    # shorten the datasets to 2 X 400, or 2 x 279 samples, in order to make them comparable with the visual stimulation task:\n",
    "    Nshort = 279\n",
    "    list1 = np.ndarray.tolist(np.arange(Nshort))\n",
    "    list2 = np.ndarray.tolist(1200+np.arange(Nshort))\n",
    "    indexes = list1 + list2       \n",
    "    data1 = data1[indexes,:]\n",
    "        \n",
    "    matMI = np.zeros((Nvars,Nvars))\n",
    "\n",
    "    for ix in np.arange(Nvars):\n",
    "        for jx in np.arange(ix+1,Nvars):\n",
    "            matMI[ix,jx] = calc_MI(surrogate_subject[:,ix], surrogate_subject[:,jx], bins)\n",
    "            matMI[jx,ix] = matMI[ix,jx]    \n",
    "    mutual_information_all_surrogate[ind,:,:] = matMI  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "foldername = 'output_second_level/'\n",
    "if not os.path.exists(foldername):\n",
    "    os.makedirs(foldername)\n",
    "    \n",
    "filename = foldername + 'mutual_information_all_surrogate_' + condition + '_800samples.mat'\n",
    "scipy.io.savemat(filename, {'mutual_information_all_surrogate': mutual_information_all_surrogate})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
